{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Spark\n",
    "Name: D.Saravanan <br>\n",
    "Date: 19/09/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   2 raman supergroup     993700 2020-09-16 10:11 /user/raman/Spark/amusements_in_mathematics.txt\r\n",
      "-rw-r--r--   2 raman supergroup       4351 2020-09-16 10:20 /user/raman/Spark/iris.csv\r\n",
      "-rw-r--r--   2 raman supergroup        539 2020-09-18 12:19 /user/raman/Spark/textfile.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/raman/Spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Find length of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfile = sc.textFile(\"/user/raman/Spark/textfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When a man says, \"I have never solved a puzzle in my life,\" it is',\n",
       " 'difficult to know exactly what he means, for every intelligent',\n",
       " 'individual is doing it every day. The unfortunate inmates of our lunatic',\n",
       " 'asylums are sent there expressly because they cannot solve',\n",
       " 'puzzles--because they have lost their powers of reason. If there were no',\n",
       " 'puzzles to solve, there would be no questions to ask; and if there were',\n",
       " 'no questions to be asked, what a world it would be! We should all be',\n",
       " 'equally omniscient, and conversation would be useless and idle.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfile.flatMap(lambda line:line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'a',\n",
       " 'man',\n",
       " 'says,',\n",
       " '\"I',\n",
       " 'have',\n",
       " 'never',\n",
       " 'solved',\n",
       " 'a',\n",
       " 'puzzle',\n",
       " 'in',\n",
       " 'my',\n",
       " 'life,\"',\n",
       " 'it',\n",
       " 'is',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'know',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'he',\n",
       " 'means,',\n",
       " 'for',\n",
       " 'every',\n",
       " 'intelligent',\n",
       " 'individual',\n",
       " 'is',\n",
       " 'doing',\n",
       " 'it',\n",
       " 'every',\n",
       " 'day.',\n",
       " 'The',\n",
       " 'unfortunate',\n",
       " 'inmates',\n",
       " 'of',\n",
       " 'our',\n",
       " 'lunatic',\n",
       " 'asylums',\n",
       " 'are',\n",
       " 'sent',\n",
       " 'there',\n",
       " 'expressly',\n",
       " 'because',\n",
       " 'they',\n",
       " 'cannot',\n",
       " 'solve',\n",
       " 'puzzles--because',\n",
       " 'they',\n",
       " 'have',\n",
       " 'lost',\n",
       " 'their',\n",
       " 'powers',\n",
       " 'of',\n",
       " 'reason.',\n",
       " 'If',\n",
       " 'there',\n",
       " 'were',\n",
       " 'no',\n",
       " 'puzzles',\n",
       " 'to',\n",
       " 'solve,',\n",
       " 'there',\n",
       " 'would',\n",
       " 'be',\n",
       " 'no',\n",
       " 'questions',\n",
       " 'to',\n",
       " 'ask;',\n",
       " 'and',\n",
       " 'if',\n",
       " 'there',\n",
       " 'were',\n",
       " 'no',\n",
       " 'questions',\n",
       " 'to',\n",
       " 'be',\n",
       " 'asked,',\n",
       " 'what',\n",
       " 'a',\n",
       " 'world',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be!',\n",
       " 'We',\n",
       " 'should',\n",
       " 'all',\n",
       " 'be',\n",
       " 'equally',\n",
       " 'omniscient,',\n",
       " 'and',\n",
       " 'conversation',\n",
       " 'would',\n",
       " 'be',\n",
       " 'useless',\n",
       " 'and',\n",
       " 'idle.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda word:len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 11,\n",
       " 10,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 11,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 16,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 11,\n",
       " 3,\n",
       " 12,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda num:(num, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1),\n",
       " (1, 1),\n",
       " (3, 1),\n",
       " (5, 1),\n",
       " (2, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (1, 1),\n",
       " (6, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (6, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (9, 1),\n",
       " (2, 1),\n",
       " (4, 1),\n",
       " (7, 1),\n",
       " (4, 1),\n",
       " (2, 1),\n",
       " (6, 1),\n",
       " (3, 1),\n",
       " (5, 1),\n",
       " (11, 1),\n",
       " (10, 1),\n",
       " (2, 1),\n",
       " (5, 1),\n",
       " (2, 1),\n",
       " (5, 1),\n",
       " (4, 1),\n",
       " (3, 1),\n",
       " (11, 1),\n",
       " (7, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (7, 1),\n",
       " (7, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (9, 1),\n",
       " (7, 1),\n",
       " (4, 1),\n",
       " (6, 1),\n",
       " (5, 1),\n",
       " (16, 1),\n",
       " (4, 1),\n",
       " (4, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (2, 1),\n",
       " (7, 1),\n",
       " (2, 1),\n",
       " (5, 1),\n",
       " (4, 1),\n",
       " (2, 1),\n",
       " (7, 1),\n",
       " (2, 1),\n",
       " (6, 1),\n",
       " (5, 1),\n",
       " (5, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (9, 1),\n",
       " (2, 1),\n",
       " (4, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (5, 1),\n",
       " (4, 1),\n",
       " (2, 1),\n",
       " (9, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (6, 1),\n",
       " (4, 1),\n",
       " (1, 1),\n",
       " (5, 1),\n",
       " (2, 1),\n",
       " (5, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (6, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (7, 1),\n",
       " (11, 1),\n",
       " (3, 1),\n",
       " (12, 1),\n",
       " (5, 1),\n",
       " (2, 1),\n",
       " (7, 1),\n",
       " (3, 1),\n",
       " (5, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reduceByKey(lambda m,n:m+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 14),\n",
       " (2, 25),\n",
       " (6, 9),\n",
       " (10, 1),\n",
       " (16, 1),\n",
       " (12, 1),\n",
       " (1, 3),\n",
       " (3, 10),\n",
       " (5, 16),\n",
       " (9, 4),\n",
       " (7, 9),\n",
       " (11, 3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.saveAsTextFile(\"hdfs:///user/raman/Spark/Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - raman supergroup          0 2020-09-19 09:48 /user/raman/Spark/Result\r\n",
      "-rw-r--r--   2 raman supergroup     993700 2020-09-16 10:11 /user/raman/Spark/amusements_in_mathematics.txt\r\n",
      "-rw-r--r--   2 raman supergroup       4351 2020-09-16 10:20 /user/raman/Spark/iris.csv\r\n",
      "-rw-r--r--   2 raman supergroup        539 2020-09-18 12:19 /user/raman/Spark/textfile.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/raman/Spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   2 raman supergroup          0 2020-09-19 09:48 /user/raman/Spark/Result/_SUCCESS\r\n",
      "-rw-r--r--   2 raman supergroup         47 2020-09-19 09:48 /user/raman/Spark/Result/part-00000\r\n",
      "-rw-r--r--   2 raman supergroup         45 2020-09-19 09:48 /user/raman/Spark/Result/part-00001\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/raman/Spark/Result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-19 09:48:17,120 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "(4, 14)\n",
      "(2, 25)\n",
      "(6, 9)\n",
      "(10, 1)\n",
      "(16, 1)\n",
      "(12, 1)\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/raman/Spark/Result/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-19 09:48:20,029 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "(1, 3)\n",
      "(3, 10)\n",
      "(5, 16)\n",
      "(9, 4)\n",
      "(7, 9)\n",
      "(11, 3)\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/raman/Spark/Result/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write a script to find species wise count of sepals in iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - raman supergroup          0 2020-09-19 09:48 /user/raman/Spark/Result\n",
      "-rw-r--r--   2 raman supergroup     993700 2020-09-16 10:11 /user/raman/Spark/amusements_in_mathematics.txt\n",
      "-rw-r--r--   2 raman supergroup       4351 2020-09-16 10:20 /user/raman/Spark/iris.csv\n",
      "-rw-r--r--   2 raman supergroup        539 2020-09-18 12:19 /user/raman/Spark/textfile.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/raman/Spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"/user/raman/Spark/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|_c0|Sepal.Length|Sepal.Width|Petal.Length|Petal.Width|Species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|  4|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|  5|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|  6|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|  7|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|  8|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|  9|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "| 10|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Sepal.Length',\n",
       " 'Sepal.Width',\n",
       " 'Petal.Length',\n",
       " 'Petal.Width',\n",
       " 'Species']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Sepal.Length: string (nullable = true)\n",
      " |-- Sepal.Width: string (nullable = true)\n",
      " |-- Petal.Length: string (nullable = true)\n",
      " |-- Petal.Width: string (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Sepal.Length',\n",
       " 'Sepal.Width',\n",
       " 'Petal.Length',\n",
       " 'Petal.Width',\n",
       " 'Species']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('Sepal.Length', 'string'),\n",
       " ('Sepal.Width', 'string'),\n",
       " ('Petal.Length', 'string'),\n",
       " ('Petal.Width', 'string'),\n",
       " ('Species', 'string')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Petal.Length', 'Petal.Width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+\n",
      "|_c0|Sepal.Length|Sepal.Width|Species|\n",
      "+---+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5| setosa|\n",
      "|  2|         4.9|        3.0| setosa|\n",
      "|  3|         4.7|        3.2| setosa|\n",
      "|  4|         4.6|        3.1| setosa|\n",
      "|  5|         5.0|        3.6| setosa|\n",
      "+---+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+\n",
      "|_c0|Sepal.Length|Sepal.Width|Species|\n",
      "+---+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5| setosa|\n",
      "|  2|         4.9|        3.0| setosa|\n",
      "|  3|         4.7|        3.2| setosa|\n",
      "|  4|         4.6|        3.1| setosa|\n",
      "|  5|         5.0|        3.6| setosa|\n",
      "+---+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Sepal.Length', 'Sepal_Length').withColumnRenamed('Sepal.Width', 'Sepal_Width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+\n",
      "|_c0|Sepal_Length|Sepal_Width|Species|\n",
      "+---+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5| setosa|\n",
      "|  2|         4.9|        3.0| setosa|\n",
      "|  3|         4.7|        3.2| setosa|\n",
      "|  4|         4.6|        3.1| setosa|\n",
      "|  5|         5.0|        3.6| setosa|\n",
      "+---+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn('Sepal_Length', df['Sepal_Length'].cast(DoubleType()))\n",
    "df = df.withColumn('Sepal_Width', df['Sepal_Width'].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('Sepal_Length', 'double'),\n",
       " ('Sepal_Width', 'double'),\n",
       " ('Species', 'string')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+\n",
      "|_c0|Sepal_Length|Sepal_Width|Species|\n",
      "+---+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5| setosa|\n",
      "|  2|         4.9|        3.0| setosa|\n",
      "|  3|         4.7|        3.2| setosa|\n",
      "|  4|         4.6|        3.1| setosa|\n",
      "|  5|         5.0|        3.6| setosa|\n",
      "+---+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   Species|count|\n",
      "+----------+-----+\n",
      "| virginica|   50|\n",
      "|versicolor|   50|\n",
      "|    setosa|   50|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Species').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|   Species| sum(Sepal_Length)|  sum(Sepal_Width)|\n",
      "+----------+------------------+------------------+\n",
      "| virginica| 329.3999999999999|             148.7|\n",
      "|versicolor|             296.8|138.50000000000003|\n",
      "|    setosa|250.29999999999998|171.40000000000003|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Species').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------------+\n",
      "|   Species|max(Sepal_Length)|max(Sepal_Width)|\n",
      "+----------+-----------------+----------------+\n",
      "| virginica|              7.9|             3.8|\n",
      "|versicolor|              7.0|             3.4|\n",
      "|    setosa|              5.8|             4.4|\n",
      "+----------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Species').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----------------+\n",
      "|   Species|min(Sepal_Length)|min(Sepal_Width)|\n",
      "+----------+-----------------+----------------+\n",
      "| virginica|              4.9|             2.2|\n",
      "|versicolor|              4.9|             2.0|\n",
      "|    setosa|              4.3|             2.3|\n",
      "+----------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Species').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+\n",
      "|   Species|avg(Sepal_Length)|  avg(Sepal_Width)|\n",
      "+----------+-----------------+------------------+\n",
      "| virginica|6.587999999999998|2.9739999999999998|\n",
      "|versicolor|            5.936|2.7700000000000005|\n",
      "|    setosa|5.005999999999999| 3.428000000000001|\n",
      "+----------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Species').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------------+------------------+------------------+------------------+\n",
      "|   Species|No. of records|    Sum of records|Maximum of records|Minimum of records|Average of records|\n",
      "+----------+--------------+------------------+------------------+------------------+------------------+\n",
      "| virginica|            50| 329.3999999999999|               7.9|               4.9| 6.587999999999998|\n",
      "|versicolor|            50|             296.8|               7.0|               4.9|             5.936|\n",
      "|    setosa|            50|250.29999999999998|               5.8|               4.3| 5.005999999999999|\n",
      "+----------+--------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "df.groupby('Species').agg(f.count('Sepal_Length').alias('No. of records'), \\\n",
    "                          f.sum('Sepal_Length').alias('Sum of records'), \\\n",
    "                          f.max('Sepal_Length').alias('Maximum of records'), \\\n",
    "                          f.min('Sepal_Length').alias('Minimum of records'), \\\n",
    "                          f.mean('Sepal_Length').alias('Average of records')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------------+------------------+------------------+------------------+\n",
      "|   Species|No. of records|    Sum of records|Maximum of records|Minimum of records|Average of records|\n",
      "+----------+--------------+------------------+------------------+------------------+------------------+\n",
      "| virginica|            50|             148.7|               3.8|               2.2|2.9739999999999998|\n",
      "|versicolor|            50|138.50000000000003|               3.4|               2.0|2.7700000000000005|\n",
      "|    setosa|            50|171.40000000000003|               4.4|               2.3| 3.428000000000001|\n",
      "+----------+--------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "df.groupby('Species').agg(f.count('Sepal_Width').alias('No. of records'), \\\n",
    "                          f.sum('Sepal_Width').alias('Sum of records'), \\\n",
    "                          f.max('Sepal_Width').alias('Maximum of records'), \\\n",
    "                          f.min('Sepal_Width').alias('Minimum of records'), \\\n",
    "                          f.mean('Sepal_Width').alias('Average of records')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
